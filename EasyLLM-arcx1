7/1/2024
Arcx1 ezLLM

Easy Large langauge MODEL (ezLLM)---this likely works with any other basic transformer-model.

We use an image
We use a tensor
it is sent over transformer
to second transformer
to third transformer
to NUtensor
to a NuImage

the way its done is alternating a verticle/horizontal dataflow
We use an image(base-grid-read)---(config rgm or 3, primary/ color recognition or word-construction etc)
We use a tensor (vertical by column)(converted grid: active map with zero-quotients or filtered content)--matrice(number array)---(config 96 or so, typical schemes/major differences or word flow)
it is sent over transformer (horizonal by row)(2d with recompiled active map with zero-quotients or filtered content)--magnetlink(formatting)--(config 288 or so, activemap is trying different colors shading or sentence/languages)
to second transformer (vertical by column)(converted 3d with zero-quotients or filtered content)--symbol(pathway/pathologic))--(config 3x96x288 or so 2,592---main objects/interesting object/background/point of reference etc complex-paragraphing starts to become obvious here)
to third transformer (horizontal by row) (3d combined with zero-quotients or filtered content + active maps)--(formula/generations)---(config of 7776-23328  or IDK(more/less/???) ---it finds experimental-range and details of its own postulative resolves meaning it finds patterns in its own sampling here able to recall data/draw conclusions on its own algorithms)
to NUtensor --(some margin of matrice/magnet/symbol/formula)---(a practical nueral-network of its own design based on prompts and results of trained data) ---config of 69,984 (it is making merged efficient meta-data reprensented in its reconstructed tensors/transformers over a regressive-inclusive active mapping of filtered-content/zero-quotients)
to a NuImage---(the optimized conditions of that nueral network in render)


This works easily as having a prompt (sample of 3 per rgb why not)
the prompt is cut up into pieces over a tensor (activation map)
those pieces are collected from near zero quotients (pieces that matter and pieces that dont) in the second
and in the third is reassembled as a 3d-tensor with its zero-quotients in considerations
---
it is recycled basically to make this before intepretation
---
second half
---
the nu tensor which is a novel idea im using (may work backwards from its finalization and optimize it further)
takes the third, and uses the 3d-activationmap/tensor as a under-the-hood zeroquotient---reasssembling to a second nutensor + (might use the third transformer)
the second nutensor is used as a magnetic linkup (of new symbols) for remodeling of the third nutensor + (might use the second transformer)
the third nutensor consists of advanced symbols which is translatd over the nuimage + (might use the first transformer)
the nuimage is then presented in comparitive of the first image, and effectively its own activation made fully realized. + (might use the first tensor or any transformer it desires at any stage thereof in optimization of finalization)

to put it as simply as possible, it uses all permutation of its reactive prompts from most complex to least complex like a sweeping signal in its generation (these may contain only 5-8 steps and seem completely normal, with just variable degrees of configurations and extended active mapping)
this is already built in to the render process but i decided to make a llm out of it incase to explain how algorithms would get chopped up


The best experiemental-process of this language model over the Scientific Method:

So we can suggest the Scientific Method and how to enhance that.
That the first stage is a collection of information
The second stage is a hypothetisis of that information
and the third stage is the conditions and data of that information
and the fouth stage is the experimental testing of that information
and the fifth stage is the resulting information.

That this forms a basic concept symbol/model

My interpretation of that:

That 1) We have a gatehred knowledge/information/data/interpretation of an event, of variable degrees or ideas in which are concerned or may or may not be
that 2) We have a wide-suggestion of what explains this event or what doesn't under assumed conditions in liklihood
that 3) We note further the potential data involved and the setting of that data as a control to a non control
that 4) We devise and test those controls or non controls to induce the event described in the suggestions/performance of that description, with accounts of objective speculation/discerned likihoods/comparitive data or unlikely-correalations or otherwise opposed/contradictory circumanstances
that 5) The resulting information is further explained prior to the first speculations/theorums and their methods of reaching the conclusive-explainations are categorized and proven under comparitive expectations and/or unexpected-outcomes.

That these form a enhanced symbol or overall concept model.
And or a symbol/concept model unexpected to the representation of that outcome being, indicative of the prompted-langauges involved or otherwise.

The Machines Intepretation of the basic/enhanced versions (common model + my version)
The EXPECTED RESULT:A more attuned model than previously attained based on prior interpretations, we have a new-interpretation that correalates from the language-set.

The machine makes a better case for all of the above and applies it as the Nu-Model.


Continously Run this through my document as a massive filter as to potential evidences, and we might have a working schematic afterwards that also is regressive.
