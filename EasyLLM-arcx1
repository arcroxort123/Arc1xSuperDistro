7/1/2024
Arcx1 ezLLM

Easy Large langauge MODEL (ezLLM)

We use an image
We use a tensor
it is sent over transformer
to second transformer
to third transformer
to NUtensor
to a NuImage

This works easily as having a prompt (sample of 3 per rgb why not)
the prompt is cut up into pieces over a tensor (activation map)
those pieces are collected from near zero quotients (pieces that matter and pieces that dont) in the second
and in the third is reassembled as a 3d-tensor with its zero-quotients in considerations
---
it is recycled basically to make this before intepretation
---
second half
---
the nu tensor which is a novel idea im using
takes the third, and uses the 3d-activationmap/tensor as a under-the-hood zeroquotient---reasssembling to a second nutensor
the second nutensor is used as a magnetic linkup (of new symbols) for remodeling of the third nutensor
the third nutensor consists of advanced symbols which is translatd over the nuimage
the nuimage is then presented in comparitive of the first image, and effectively its own activation made fully realized.

this is already built in to the render process but i decided to make a llm out of it incase to explain how algorithms would get chopped up


The best experiemental-process of this language model over the Scientific Method:

So we can suggest the Scientific Method and how to enhance that.
That the first stage is a collection of information
The second stage is a hypothetisis of that information
and the third stage is the conditions and data of that information
and the fouth stage is the experimental testing of that information
and the fifth stage is the resulting information.

That this forms a basic concept symbol/model

My interpretation of that:

That 1) We have a gatehred knowledge/information/data/interpretation of an event, of variable degrees or ideas in which are concerned or may or may not be
that 2) We have a wide-suggestion of what explains this event or what doesn't under assumed conditions in liklihood
that 3) We note further the potential data involved and the setting of that data as a control to a non control
that 4) We devise and test those controls or non controls to induce the event described in the suggestions/performance of that description, with accounts of objective speculation/discerned likihoods/comparitive data or unlikely-correalations or otherwise opposed/contradictory circumanstances
that 5) The resulting information is further explained prior to the first speculations/theorums and their methods of reaching the conclusive-explainations are categorized and proven under comparitive expectations and/or unexpected-outcomes.

That these form a enhanced symbol or overall concept model.
And or a symbol/concept model unexpected to the representation of that outcome being, indicative of the prompted-langauges involved or otherwise.

The Machines Intepretation of the basic/enhanced versions (common model + my version)
The EXPECTED RESULT:A more attuned model than previously attained based on prior interpretations, we have a new-interpretation that correalates from the language-set.

The machine makes a better case for all of the above.
